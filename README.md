# üß† AI Medical Voicebot

An advanced multimodal AI-powered medical voicebot that combines voice, image, and text inputs to generate real-time, professional medical insights.

---

## üìå Features

* üéôÔ∏è **Voice Input & Transcription**: Converts speech into text using **Whisper-large-v3** via **GROQ API**
* üñºÔ∏è **Image Analysis**: Detects medical conditions using **LLaMA-3.2-90B Vision model**
* üí¨ **AI Medical Response**: Generates accurate, professional feedback in 2‚Äì3 sentences
* üîä **Text-to-Speech (TTS)**: Converts generated text into audio using **ElevenLabs** and **gTTS**
* üåê **Multimodal Interface**: Supports voice, image, and text inputs in one seamless UI
* üñ•Ô∏è **Gradio UI**: Simple and interactive interface for users

---

## üõ†Ô∏è Tech Stack

| Category             | Tools & Services            |
| -------------------- | --------------------------- |
| Programming Language | Python                      |
| UI Framework         | Gradio                      |
| Speech-to-Text       | GROQ API (Whisper-large-v3) |
| Image Analysis       | LLaMA-3.2-90B Vision Model  |
| Text-to-Speech (TTS) | ElevenLabs, gTTS            |
| Environment Handling | python-dotenv               |

---

## üìÅ Project Structure

```
ai-doctor-voicebot/
‚îú‚îÄ‚îÄ app.py                 # Main Gradio interface
‚îú‚îÄ‚îÄ whisper_utils.py       # Voice transcription logic
‚îú‚îÄ‚îÄ vision_utils.py        # Image analysis with LLaMA
‚îú‚îÄ‚îÄ tts_utils.py           # Text-to-Speech functions
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env.example           # Example environment variables
‚îî‚îÄ‚îÄ README.md              # Project documentation
```

---

## üîß Setup Instructions

1. **Clone the Repository**

```bash
git clone https://github.com/danishali22/ai-doctor-voicebot.git
cd ai-doctor-voicebot
```

2. **Install Dependencies**

```bash
pip install -r requirements.txt
```

3. **Configure Environment Variables**

```bash
cp .env.example .env
# Fill in your GROQ API key, ElevenLabs key, etc.
```

4. **Run the App**

```bash
python app.py
```

---

## ü§ñ How It Works

1. User speaks or uploads a medical image.
2. Voice is transcribed using Whisper via GROQ API.
3. Image is analyzed via LLaMA-3.2-90B Vision model.
4. A short, concise medical insight is generated.
5. Response is read aloud using TTS.

---

## üìö What I Learned

* Efficient use of **multimodal AI** (text + vision + audio)
* Integration of **GROQ Whisper** and **LLaMA vision models**
* Working with **real-time audio and speech generation APIs**
* Building clean, user-friendly Gradio apps

---

## üôå Feedback

If you find this helpful or want to collaborate, feel free to open an issue or connect!

---

**GitHub:** [danishali22](https://github.com/danishali22)

---

> ‚ö†Ô∏è This project is for educational/demo purposes. Not intended for real-world diagnosis or treatment.
